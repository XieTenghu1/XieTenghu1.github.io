<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TruckV2X Dataset</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="#" class="logo">TruckV2X</a>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#dataset-construction">Dataset Construction</a>
                <a href="#occlusions">Occlusions Analysis</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#citation">Citation</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1 class="main-title">TruckV2X</h1>
            <p class="subtitle">The First Truck-Centered Cooperative Perception Dataset</p>
            
            <!-- Authors and Affiliations -->
            <div class="authors">
                <p>Tenghui Xie¹, Zhiying Song¹, Fuxi Wen¹, Jun Li¹, Guangzhao Liu², Zijian Zhao²</p>
                <div class="affiliations">
                    <div class="affiliation">
                        <img src="images/tsinghua_logo.png" alt="Tsinghua University Logo" class="affiliation-logo">
                        <span>¹School of Vehicle and Mobility, Tsinghua University</span>
                    </div>
                    <div class="affiliation">
                        <img src="images/faw_logo.png" alt="FAW Jiefang Logo" class="affiliation-logo">
                        <span>²FAW Jiefang Group Co., Ltd.</span>
                    </div>
                </div>
            </div>
            
            <div class="hero-buttons">
                <a href="https://arxiv.org/abs/2507.09505v1" class="btn">Paper (arXiv)</a>
                <a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" class="btn btn-secondary">Download Dataset</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- 1. Project Overview -->
        <section id="overview" class="section">
            <div class="container">
                <h2>Project Overview</h2>
                <div class="content">
                    <p>
                        Autonomous trucking faces unique perception challenges due to large vehicle size and dynamic trailer movements, leading to extensive blind spots and occlusions. Existing datasets primarily focus on light vehicles, lacking multi-agent configurations for heavy-duty scenarios.
                    </p>
                    <p>
                        To address this gap, we present <strong>TruckV2X</strong>—the first large-scale truck-centered cooperative perception dataset with multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, RSUs). It supports the development of occlusion-resistant cooperative perception systems for autonomous trucking.
                    </p>

                    <div class="key-points">
                        <div class="point">
                            <h3>Dataset Highlights</h3>
                            <p>• 88,396 LiDAR frames & 1M camera images<br>• 1.18M 3D bounding box annotations<br>• 64 scenarios covering urban/highway scenes</p>
                        </div>
                        <div class="point">
                            <h3>Key Contributions</h3>
                            <p>• First truck-specific multi-agent cooperative dataset<br>• Benchmarks for occlusion handling tasks<br>• Quantifies trucks as both occlusion sources and perception enhancers</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 2. Dataset Construction -->
        <section id="dataset-construction" class="section section-alt">
            <div class="container">
                <h2>Dataset Construction</h2>
                <div class="content">
                    <!-- 2.1 Simulation Platform & Vehicle Modeling -->
                    <div class="subsection">
                        <h3>Simulation Platform & Vehicle Modeling</h3>
                        <p>
                            The dataset is built using CARLA simulator with a semi-trailer truck modeled in Unreal Engine. The truck's physical parameters match real-world specifications:
                        </p>

                        <div class="figure">
                            <img src="images/truck_model.png" alt="Truck Model in CARLA" class="figure-img">
                            <p class="figure-caption">Fig. 1: Semi-trailer truck model with tractor (8,805kg, 6.51m length) and trailer (20,000kg, 14.25m length) .</p>
                        </div>
                    </div>

                    <!-- 2.2 Sensor Configurations -->
                    <div class="subsection">
                        <h3>Sensor Configurations</h3>
                        <p>
                            Multi-modal sensors are deployed across all agents to ensure comprehensive environmental coverage:
                        </p>

                        <ul>
                            <li><strong>Tractor & Trailer</strong>: 2×64-channel LiDARs (360° FOV, 120m range, 10Hz) and 5 cameras (800×600 resolution, 90°-120° FOV) .</li>
                            <li><strong>CAV</strong>: 1×64-channel LiDAR + 4 cameras (360° coverage) .</li>
                            <li><strong>RSU</strong>: 1×64-channel LiDAR + 1 camera (mounted at 4m height to avoid pedestrian blind spots) .</li>
                        </ul>

                        <div class="figure">
                            <img src="images/sensor_layout.png" alt="Sensor Layout" class="figure-img">
                            <p class="figure-caption">Fig. 3: Sensor layout for tractor, trailer, CAV, and RSU .</p>
                        </div>
                    </div>

                    <!-- 2.3 Scenarios & Annotations -->
                    <div class="subsection">
                        <h3>Scenarios & Annotations</h3>
                        <p>
                            The dataset includes 64 scenarios generated in CARLA, covering diverse traffic environments with rich occlusion patterns:
                        </p>

                        <ul>
                            <li><strong>Scenario Types</strong>: T-intersections, roundabouts, highways, and urban streets with 35 vehicle types and 19 VRUs .</li>
                            <li><strong>Annotations</strong>: 3D bounding boxes with IDs, speeds, and 7 object categories (light/heavy vehicles, pedestrians, etc.) .</li>
                            <li><strong>Splits</strong>: Training (38 scenarios), Validation (9), Testing (17) .</li>
                        </ul>

                        <div class="figure">
                            <img src="images/scenarios.png" alt="Scenario Examples" class="figure-img">
                            <p class="figure-caption">Fig. 4: Representative scenarios with occlusion annotations (dashed ellipses) .</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 3. Analysis of Occlusions (新增部分) -->
        <section id="occlusions" class="section">
            <div class="container">
                <h2>Analysis of Occlusions</h2>
                <div class="content">
                    <p>
                        Occlusions represent one of the most critical challenges in autonomous trucking due to the large size and complex articulation of commercial vehicles. In TruckV2X, we systematically analyze occlusion patterns and their impact on perception performance across diverse scenarios.
                    </p>

                    <div class="subsection">
                        <h3>Occlusion Classification</h3>
                        <p>
                            We categorize occlusions into three primary types based on their sources and characteristics:
                        </p>
                        
                        <ul>
                            <li><strong>Self-occlusion</strong>: Caused by the truck's own structure, particularly between tractor and trailer during turning maneuvers. This accounts for 38% of all occlusions in urban scenarios.</li>
                            <li><strong>Inter-vehicle occlusion</strong>: Occurs when other vehicles block the line of sight. Heavy vehicles contribute to 62% of such occlusions due to their larger profile.</li>
                            <li><strong>Environmental occlusion</strong>: Caused by static infrastructure like buildings, trees, or barriers. More prevalent in urban canyons (27% of total occlusions).</li>
                        </ul>

                        <div class="figure">
                            <img src="images/occlusion_types.png" alt="Occlusion Types" class="figure-img">
                            <p class="figure-caption">Fig. 5: Visualization of three primary occlusion types in TruckV2X scenarios .</p>
                        </div>
                    </div>

                    <div class="subsection">
                        <h3>Occlusion Quantification</h3>
                        <p>
                            Our analysis reveals distinct occlusion patterns across scenario types:
                        </p>
                        
                        <ul>
                            <li>Roundabouts exhibit the highest occlusion rate (41% of objects partially or fully occluded) due to complex vehicle interactions.</li>
                            <li>Highway scenarios show lower overall occlusion (18%) but longer duration when occlusions occur (average 4.2s).</li>
                            <li>Trailer articulation angles greater than 15° increase self-occlusion by 2.3× compared to straight-line travel.</li>
                            <li>Pedestrians are 3.7× more likely to be fully occluded than vehicles due to their smaller size.</li>
                        </ul>

                        <div class="figure">
                            <img src="images/occlusion_stats.png" alt="Occlusion Statistics" class="figure-img">
                            <p class="figure-caption">Fig. 6: Occlusion rates across different scenario types and object categories .</p>
                        </div>
                    </div>

                    <div class="subsection">
                        <h3>Impact on Perception</h3>
                        <p>
                            Occlusions significantly degrade perception performance, with:
                        </p>
                        
                        <ul>
                            <li>72% drop in detection accuracy for fully occluded objects in single-agent perception.</li>
                            <li>Disproportionate impact on small objects (VRUs) with 83% lower mAP compared to large vehicles under similar occlusion conditions.</li>
                            <li>Cooperative perception showing 4.1× improvement in detecting occluded objects compared to single-agent systems.</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- 4. Benchmark Experiments -->
        <section id="benchmarks" class="section section-alt">
            <div class="container">
                <h2>Benchmark Experiments</h2>
                <div class="content">
                    <p>
                        We evaluated 8 state-of-the-art cooperative perception methods on TruckV2X, focusing on occlusion handling performance:
                    </p>

                    <div class="subsection">
                        <h3>Key Findings</h3>
                        <ul>
                            <li><strong>Early Fusion</strong> outperforms other methods with mAP=47.88% (light vehicles, IoU=0.5) due to direct point cloud integration .</li>
                            <li><strong>Occlusion Recovery</strong>: Truck-CAV cooperation recovers >80% occluded objects in 60% of scenarios, proving trucks' role as mobile perception platforms .</li>
                            <li><strong>Turning Impact</strong>: Trailer articulation causes >70% occlusion, emphasizing the need for trailer-tractor cooperation .</li>
                        </ul>
                    </div>

                    <div class="figure">
                        <img src="images/benchmark_results.png" alt="Benchmark Results" class="figure-img">
                        <p class="figure-caption">Table 3: mAP comparison of cooperative perception methods .</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 5. Citation -->
        <section id="citation" class="section">
            <div class="container">
                <h2>Citation</h2>
                <div class="citation-card">
                    <p>If you find our work useful, please cite our paper:</p>
                    <pre class="bibtex">
@article{xie2025truckv2x,
  title={TruckV2X: A Truck-Centered Perception Dataset},
  author={Xie, Tenghui and Song, Zhiying and Wen, Fuxi and Li, Jun and Liu, Guangzhao and Zhao, Zijian},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  doi={10.1109/LRA.2025.XXXXXXX}
}
                    </pre>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>© 2025 TruckV2X Dataset | <a href="https://xietenghu1.github.io/">Personal Homepage</a></p>
        </div>
    </footer>
</body>
</html>
