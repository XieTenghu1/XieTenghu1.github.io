<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TruckV2X Dataset</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="#" class="logo">TruckV2X</a>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#dataset-construction">Dataset Construction</a>
                <a href="#occlusions-analysis">Occlusions Analysis</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#citation">Citation</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1 class="main-title">ðŸš› TruckV2X</h1>
            <p class="subtitle">The First Truck-Centered Cooperative Perception Dataset</p>
            
            <!-- Authors and Affiliations -->
            <div class="authors">
                <p>Tenghui XieÂ¹, Zhiying SongÂ¹, Fuxi WenÂ¹(Corresponding author), Jun LiÂ¹, Guangzhao LiuÂ², Zijian ZhaoÂ²</p>
                <div class="affiliations">
                    <div class="affiliation">
                        <span>Â¹School of Vehicle and Mobility, Tsinghua University</span>
                    </div>
                    <div class="affiliation">
                        <span>Â²FAW Jiefang Group Co., Ltd.</span>
                    </div>
                </div>
            </div>
            
            <div class="hero-buttons">
                <a href="https://arxiv.org/abs/2507.09505v1" class="btn">Paper</a>
                <a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" class="btn">Dataset</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- 1. Dataset Overview -->
        <section id="overview" class="section">
            <div class="container">
                <h2>Overview</h2>
                <div class="content">
                    <p>
                        Autonomous trucking faces unique perception challenges due to large vehicle size and dynamic trailer movements, leading to extensive blind spots and occlusions. Tractor-trailer systems create persistent blind zones where structural components obstruct both self-perception and neighboring agents' sensing. Articulated motion exacerbates occlusions during maneuvers, while low-speed operation expands risk zones in mixed traffic.
                    </p>
                    <p>
                        Existing datasets primarily focus on light vehicles, lacking multi-agent configurations for heavy-duty scenarios. To address this gap, we present <strong>TruckV2X</strong>â€”the first large-scale truck-centered cooperative perception dataset with multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, RSUs). It supports the development of occlusion-resistant cooperative perception systems for autonomous trucking.
                    </p>
                    
                    <div class="key-points">
                        <div class="point">
                            <h3>Dataset Highlights</h3>
                            <p>â€¢ 88,396 LiDAR frames <br>â€¢ 1.18M 3D bounding box annotations<br>â€¢ 64 scenarios covering urban/highway scenes<br>â€¢ Multi-agent data: tractors, trailers, CAVs, RSUs</p>
                        </div>
                        <div class="point">
                            <h3>Key Contributions</h3>
                            <p>â€¢ First truck-centered multi-agent cooperative dataset<br>â€¢ Benchmarks for truck collaborative perception<br>â€¢ Quantifies trucks as occlusion sources and perception enhancers</p>
                        </div>
                    </div>

                    <!-- Figure 1: Half-column with text (left: image, right: text) -->
                    <div class="half-column-container">
                        <div class="half-column-image">
                            <img src="images/fig1.png" alt="Truck-related occlusions" class="figure-img">
                            <p class="figure-caption">Fig. 1: Illustration of truck-related occlusions.</p>
                        </div>
                        <div class="half-column-text">
                            <h3>Truck-specific Perception Challenges</h3>
                            <p>Trucks present unique perception challenges due to their large size and articulated structure. The tractor-trailer configuration creates extensive blind zones that significantly impact both self-perception and the perception capabilities of surrounding vehicles.</p>
                            <p>Our analysis shows that tractor-trailer combinations create 1.5Ã— more occluded area than passenger cars within 30m, worsening with trailer pivot (over 70% occlusion at 90Â° turns), making cooperative perception essential.</p>
                        </div>
                    </div>

                    <!-- Figure 2: Half-column reversed (left: text, right: image) -->
                    <div class="half-column-container reverse">
                        <div class="half-column-image">
                            <img src="images/fig2.png" alt="Dataset comparison" class="figure-img">
                            <p class="figure-caption">Fig. 2: Datasets available for the perception of autonomous driving.</p>
                        </div>
                        <div class="half-column-text">
                            <h3>Dataset Positioning</h3>
                            <p>TruckV2X is the first multi-agent collaborative perception dataset specifically designed for heavy vehicles. TruckV2X fills a critical gap in existing autonomous driving datasets by focusing specifically on heavy-duty vehicles and their unique perception challenges.</p>
                            <p>Unlike previous datasets that primarily focus on passenger vehicles, TruckV2X incorporates multi-agent cooperation scenarios involving tractors, trailers, other connected vehicles, and road infrastructure, creating a comprehensive platform for developing cooperative perception algorithms.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 2. Dataset Construction -->
        <section id="dataset-construction" class="section section-alt">
            <div class="container">
                <h2>Dataset Construction</h2>
                <div class="content">
                    <!-- 2.1 Vehicle & Sensor Configurations -->
                    <div class="subsection">
                        <h3>Vehicle & Sensor Configurations</h3>
                        <p>
                            TruckV2X integrates a high-fidelity semi-trailer truck model into CARLA via Unreal Engine, with physics-based kinematic constraints for realistic articulation. Each agent is equipped with multi-modal sensors:
                        </p>

                        <!-- Figure 3: Half-column with text -->
                        <div class="half-column-container">
                            <div class="half-column-image">
                                <img src="images/fig3.png" alt="Sensor configurations" class="figure-img">
                                <p class="figure-caption">Fig. 3: Illustration of sensor configurations for the agents.</p>
                            </div>
                            <div class="half-column-text">
                                <h3>Multi-modal Sensing Architecture</h3>
                                <p><strong>Tractor/Trailer</strong>: 2Ã—64-channel LiDARs (360Â° FOV, 120m range, 10Hz) and 5 cameras (800Ã—600 resolution, 90Â°-120Â° FOV).<p>
                                <p><strong>CAV</strong>: 1Ã—64-channel LiDAR + 4 cameras (360Â° coverage).<p>
                                <p><strong>RSU</strong>: 1Ã—64-channel LiDAR + 1 camera (mounted at 4m height to minimize pedestrian blind spots).<p>
                            </div>
                        </div>
                    </div>

                    <!-- 2.2 Scenarios & Data Statistics -->
                    <div class="subsection">
                        <h3>Scenarios & Data Statistics</h3>
                        <p>
                            The dataset was built using CARLA, with 64 scenarios generated by adding controlled random vehicles and pedestrians. Scenarios cover diverse environments: country roads, urban streets, highways, and intersections (T-shaped, four-way, roundabouts).
                        </p>

                        <div class="figure">
                            <img src="images/fig4.png" alt="Representative scenarios" class="figure-img">
                            <p class="figure-caption">Fig. 4: Representative scenarios with occlusion annotations (dashed ellipses denote blind zones).</p>
                        </div>

                        <div class="figure">
                            <img src="images/fig5.png" alt="Data statistics" class="figure-img">
                            <p class="figure-caption">Fig. 5: (a) Frame counts per scenario. (b) Distance statistics between agents. (c) Speed statistics of agents. (d) Articulation angle counts between the tractor and trailer. (e) Object category counts. (f) Statistics of co-visible objects among agents. (g) Distance statistics between objects and agents. (h) Speed statistics of objects.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 3. Analysis of Occlusions -->
        <section id="occlusions-analysis" class="section">
            <div class="container">
                <h2>Occlusions Analysis</h2>
                <div class="content">
                    <p>
                        Occlusions are a critical challenge in truck perception due to large size and articulation. TruckV2X captures extensive occlusion scenarios, with quantitative analysis revealing unique patterns:
                    </p>

                    <div class="figure">
                        <img src="images/fig6.png" alt="Occlusion distribution" class="figure-img">
                        <p class="figure-caption">Fig. 6: Distribution of objects occluded for different agents in a 120m range. Distribution of objects occluded (a) from truck; (b) from CAV by truck; (c) from RSU by truck; (d) from the tractor by trailer.</p>
                    </div>

                    <div class="subsection">
                        <h3>Cooperation Benefits: Occlusion Recovery Rate (ORR)</h3>
                        <p>
                            Cooperative perception mitigates occlusions. ORR is defined as the proportion of an ego agent's occluded objects detected by cooperators (range 0-1). Key findings:
                        </p>
                        <ul>
                            <li>Truck-CAV cooperation recovers >80% occluded objects in 60% of scenarios (short distances improve performance).</li>
                            <li>RSUs provide stable mid-range support, while trucks act as mobile platforms to reduce blind spots during maneuvers.</li>
                        </ul>

                        <div class="figure">
                            <img src="images/fig7.png" alt="ORR temporal evolution" class="figure-img">
                            <p class="figure-caption">Fig. 7: The temporal evolution of occlusion recovery rates (ORR) for CAV provided by truck and CAV, combined with the distances between truck, RSU, and CAV over time, in the T-intersection scenario (left).</p>
                        </div>

                        <!-- Figure 8: Half-column reversed -->
                        <div class="half-column-container reverse">
                            <div class="half-column-image">
                                <img src="images/fig8.png" alt="ORR distribution" class="figure-img">
                                <p class="figure-caption">Fig. 8: Frequency distribution of occlusion recovery rates (ORR) across various ego and cooperator configurations, with superimposed probability density distributions of inter-agent distances stratified by ORR intervals.</p>
                            </div>
                            <div class="half-column-text">
                                <h3>Occlusion Recovery Patterns</h3>
                                <p>Our analysis of Occlusion Recovery Rate (ORR) demonstrates how cooperative perception can significantly improve object detection in occluded scenarios.</p>
                                <p>The frequency distribution shows that different agent configurations excel at different distances, with trucks providing valuable perception support in medium-range scenarios, complementing the strengths of CAVs and RSUs.</p>
                                <p>This highlights the importance of heterogeneous multi-agent cooperation in addressing the unique occlusion challenges of trucking environments.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 4. Benchmark Experiments -->
        <section id="benchmarks" class="section section-alt">
            <div class="container">
                <h2>Benchmark Experiments</h2>
                <div class="content">
                    <p>
                        We evaluated 8 state-of-the-art cooperative perception methods on TruckV2X, focusing on 3D object detection. Metrics include Average Precision (AP) and mean AP (mAP) at IoU thresholds 0.3, 0.5, 0.7 for light vehicles, heavy vehicles, and VRUs.
                    </p>

                    <div class="subsection">
                        <h3>Key Findings</h3>
                        <ul>
                            <li><strong>Early Fusion</strong> outperforms other methods (mAP=47.88% at IoU=0.5 for truck ego) due to direct point cloud integration.</li>
                            <li><strong>Intermediate Fusion</strong> (e.g., AttFuse, V2VNet) performs well for CAV ego, matching early fusion in light vehicle detection.</li>
                            <li>VRU detection remains challenging (low AP) due to small size and sparse LiDAR points.</li>
                        </ul>
                    </div>

                    <!-- Using fig9 instead of table III -->
                    <div class="figure">
                        <img src="images/fig9.png" alt="Detection results for Truck, CAV, RSU as Ego" class="figure-img">
                        <p class="figure-caption">Fig. 9: Benchmark detection results of SOTA cooperative perception methods on TruckV2X, with truck, CAV, and RSU acting as ego in mutual collaboration, where tractor and trailer are integrated via early fusion to function as a unified agent truck.</p>
                    </div>

                    <!-- Using fig10 instead of table IV -->
                    <div class="figure">
                        <img src="images/fig10.png" alt="Detection results for Tractor as Ego with Trailer Cooperation" class="figure-img">
                        <p class="figure-caption">Fig. 10: Benchmark detection results of SOTA cooperative perception methods on TruckV2X, with tractor(ego) and trailer collaborating. </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 5. Citation -->
        <section id="citation" class="section">
            <div class="container">
                <h2>Citation</h2>
                <div class="citation-card">
                    <p>If you use TruckV2X or find our work inspiring in your research, please cite our paper:</p>
                    <pre class="bibtex">
@article{xie2025truckv2x,
  title={TruckV2X: A Truck-Centered Perception Dataset},
  author={Xie, Tenghui and Song, Zhiying and Wen, Fuxi and Li, Jun and Liu, Guangzhao and Zhao, Zijian},
  journal={arXiv preprint arXiv:2507.09505},
  year={2025}
}
                    </pre>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>Â© 2025 TruckV2X Dataset | <a href="https://xietenghu1.github.io/">Contact</a></p>
        </div>
    </footer>
</body>
</html>
