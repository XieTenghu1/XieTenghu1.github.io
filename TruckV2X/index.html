<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TruckV2X Dataset</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="#" class="logo">TruckV2X</a>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#dataset-construction">Dataset Construction</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#resources">Resources</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1>TruckV2X</h1>
            <p class="tagline">The First Truck-Centered Cooperative Perception Dataset</p>
            
            <!-- Authors and Affiliations -->
            <div class="authors">
                <p>Tenghui Xie¹, Zhiying Song¹, Fuxi Wen¹, Jun Li¹, Guangzhao Liu², Zijian Zhao²</p>
                <div class="affiliations">
                    <div class="affiliation">
                        <span>¹School of Vehicle and Mobility, Tsinghua University</span>
                    </div>
                    <div class="affiliation">
                        <span>²FAW Jiefang Group Co., Ltd.</span>
                    </div>
                </div>
            </div>
            
            <div class="hero-buttons">
                <a href="assets/truckv2x_paper.pdf" class="btn">Paper (PDF)</a>
                <a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" class="btn btn-secondary">Download Dataset</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- 1. Project Overview -->
        <section id="overview" class="section">
            <div class="container">
                <h2>Project Overview</h2>
                <div class="content">
                    <p>
                        Autonomous trucking faces unique perception challenges due to large vehicle size and dynamic trailer movements, leading to extensive blind spots and occlusions. Existing datasets primarily focus on light vehicles, lacking multi-agent configurations for heavy-duty scenarios.
                    </p>
                    <p>
                        To address this gap, we present <strong>TruckV2X</strong>—the first large-scale truck-centered cooperative perception dataset with multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, RSUs). It supports the development of occlusion-resistant cooperative perception systems for autonomous trucking.
                    </p>

                    <div class="key-points">
                        <div class="point">
                            <h3>Dataset Highlights</h3>
                            <p>• 88,396 LiDAR frames & 1M camera images<br>• 1.18M 3D bounding box annotations<br>• 64 scenarios covering urban/highway scenes</p>
                        </div>
                        <div class="point">
                            <h3>Key Contributions</h3>
                            <p>• First truck-specific multi-agent cooperative dataset<br>• Benchmarks for occlusion handling tasks<br>• Quantifies trucks as both occlusion sources and perception enhancers</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 2. Dataset Construction -->
        <section id="dataset-construction" class="section section-alt">
            <div class="container">
                <h2>Dataset Construction</h2>
                <div class="content">
                    <!-- 2.1 Simulation Platform & Vehicle Modeling -->
                    <div class="subsection">
                        <h3>Simulation Platform & Vehicle Modeling</h3>
                        <p>
                            The dataset is built using CARLA simulator with a semi-trailer truck modeled in Unreal Engine. The truck's physical parameters match real-world specifications:
                        </p>

                        <div class="figure">
                            <img src="images/truck_model.png" alt="Truck Model in CARLA" class="figure-img">
                            <p class="figure-caption">Fig. 1: Semi-trailer truck model with tractor (8,805kg, 6.51m length) and trailer (20,000kg, 14.25m length) .</p>
                        </div>
                    </div>

                    <!-- 2.2 Sensor Configurations -->
                    <div class="subsection">
                        <h3>Sensor Configurations</h3>
                        <p>
                            Multi-modal sensors are deployed across all agents to ensure comprehensive environmental coverage:
                        </p>

                        <ul>
                            <li><strong>Tractor & Trailer</strong>: 2×64-channel LiDARs (360° FOV, 120m range, 10Hz) and 5 cameras (800×600 resolution, 90°-120° FOV) .</li>
                            <li><strong>CAV</strong>: 1×64-channel LiDAR + 4 cameras (360° coverage) .</li>
                            <li><strong>RSU</strong>: 1×64-channel LiDAR + 1 camera (mounted at 4m height to avoid pedestrian blind spots) .</li>
                        </ul>

                        <div class="figure">
                            <img src="images/sensor_layout.png" alt="Sensor Layout" class="figure-img">
                            <p class="figure-caption">Fig. 3: Sensor layout for tractor, trailer, CAV, and RSU .</p>
                        </div>
                    </div>

                    <!-- 2.3 Scenarios & Annotations -->
                    <div class="subsection">
                        <h3>Scenarios & Annotations</h3>
                        <p>
                            The dataset includes 64 scenarios generated in CARLA, covering diverse traffic environments with rich occlusion patterns:
                        </p>

                        <ul>
                            <li><strong>Scenario Types</strong>: T-intersections, roundabouts, highways, and urban streets with 35 vehicle types and 19 VRUs .</li>
                            <li><strong>Annotations</strong>: 3D bounding boxes with IDs, speeds, and 7 object categories (light/heavy vehicles, pedestrians, etc.) .</li>
                            <li><strong>Splits</strong>: Training (38 scenarios), Validation (9), Testing (17) .</li>
                        </ul>

                        <div class="figure">
                            <img src="images/scenarios.png" alt="Scenario Examples" class="figure-img">
                            <p class="figure-caption">Fig. 4: Representative scenarios with occlusion annotations (dashed ellipses) .</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 3. Benchmark Experiments -->
        <section id="benchmarks" class="section">
            <div class="container">
                <h2>Benchmark Experiments</h2>
                <div class="content">
                    <p>
                        We evaluated 8 state-of-the-art cooperative perception methods on TruckV2X, focusing on occlusion handling performance:
                    </p>

                    <div class="subsection">
                        <h3>Key Findings</h3>
                        <ul>
                            <li><strong>Early Fusion</strong> outperforms other methods with mAP=47.88% (light vehicles, IoU=0.5) due to direct point cloud integration .</li>
                            <li><strong>Occlusion Recovery</strong>: Truck-CAV cooperation recovers >80% occluded objects in 60% of scenarios, proving trucks' role as mobile perception platforms .</li>
                            <li><strong>Turning Impact</strong>: Trailer articulation causes >70% occlusion, emphasizing the need for trailer-tractor cooperation .</li>
                        </ul>
                    </div>

                    <div class="figure">
                        <img src="images/benchmark_results.png" alt="Benchmark Results" class="figure-img">
                        <p class="figure-caption">Table 3: mAP comparison of cooperative perception methods .</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 4. Resources & Citation -->
        <section id="resources" class="section section-alt">
            <div class="container">
                <h2>Resources & Citation</h2>
                <div class="resources-grid">
                    <div class="resource-card">
                        <h3>Dataset Access</h3>
                        <ul>
                            <li><a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" target="_blank">Hugging Face Repository</a></li>
                            <li><a href="assets/truckv2x_paper.pdf">Full Paper (PDF)</a></li>
                        </ul>
                    </div>
                    <div class="resource-card">
                        <h3>Citation</h3>
                        <pre class="bibtex">
@article{xie2025truckv2x,
  title={TruckV2X: A Truck-Centered Perception Dataset},
  author={Xie, Tenghui and Song, Zhiying and Wen, Fuxi and Li, Jun and Liu, Guangzhao and Zhao, Zijian},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  doi={10.1109/LRA.2025.XXXXXXX}
}
                        </pre>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>© 2025 TruckV2X Dataset | <a href="https://xietenghu1.github.io/">Personal Homepage</a></p>
        </div>
    </footer>
</body>
</html>
