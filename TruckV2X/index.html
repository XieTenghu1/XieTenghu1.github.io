<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TruckV2X Dataset</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="#" class="logo">TruckV2X</a>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#dataset-construction">Dataset Construction</a>
                <a href="#occlusions-analysis">Occlusions Analysis</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#citation">Citation</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1 class="main-title">ðŸš› TruckV2X</h1>
            <p class="subtitle">The First Truck-Centered Cooperative Perception Dataset</p>
            
            <!-- Authors and Affiliations -->
            <div class="authors">
                <p>Tenghui XieÂ¹, Zhiying SongÂ¹, Fuxi WenÂ¹, Jun LiÂ¹, Guangzhao LiuÂ², Zijian ZhaoÂ²</p>
                <div class="affiliations">
                    <div class="affiliation">
                        <span>Â¹School of Vehicle and Mobility, Tsinghua University</span>
                    </div>
                    <div class="affiliation">
                        <span>Â²FAW Jiefang Group Co., Ltd.</span>
                    </div>
                </div>
            </div>
            
            <div class="hero-buttons">
                <a href="https://arxiv.org/abs/2507.09505v1" class="btn">Paper (arXiv)</a>
                <a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" class="btn btn-secondary">Download Dataset</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- 1. Project Overview -->
        <section id="overview" class="section">
            <div class="container">
                <h2>Project Overview</h2>
                <div class="content">
                    <p>
                        Autonomous trucking faces unique perception challenges due to large vehicle size and dynamic trailer movements, leading to extensive blind spots and occlusions. Existing datasets primarily focus on light vehicles, lacking multi-agent configurations for heavy-duty scenarios.
                    </p>
                    <p>
                        To address this gap, we present <strong>TruckV2X</strong>â€”the first large-scale truck-centered cooperative perception dataset with multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, RSUs). It supports the development of occlusion-resistant cooperative perception systems for autonomous trucking.
                    </p>

                    <div class="key-points">
                        <div class="point">
                            <h3>Dataset Highlights</h3>
                            <p>â€¢ 88,396 LiDAR frames & 1M camera images<br>â€¢ 1.18M 3D bounding box annotations<br>â€¢ 64 scenarios covering urban/highway scenes</p>
                        </div>
                        <div class="point">
                            <h3>Key Contributions</h3>
                            <p>â€¢ First truck-specific multi-agent cooperative dataset<br>â€¢ Benchmarks for occlusion handling tasks<br>â€¢ Quantifies trucks as both occlusion sources and perception enhancers</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 2. Dataset Construction -->
        <section id="dataset-construction" class="section section-alt">
            <div class="container">
                <h2>Dataset Construction</h2>
                <div class="content">
                    <!-- 2.1 Simulation Platform & Vehicle Modeling -->
                    <div class="subsection">
                        <h3>Simulation Platform & Vehicle Modeling</h3>
                        <p>
                            The dataset is built using CARLA simulator with a semi-trailer truck modeled in Unreal Engine. The truck's physical parameters match real-world specifications:
                        </p>

                        <div class="figure">
                            <img src="images/truck_model.png" alt="Truck Model in CARLA" class="figure-img">
                            <p class="figure-caption">Fig. 1: Semi-trailer truck model with tractor (8,805kg, 6.51m length) and trailer (20,000kg, 14.25m length).</p>
                        </div>
                    </div>

                    <!-- 2.2 Sensor Configurations -->
                    <div class="subsection">
                        <h3>Sensor Configurations</h3>
                        <p>
                            Multi-modal sensors are deployed across all agents to ensure comprehensive environmental coverage:
                        </p>

                        <ul>
                            <li><strong>Tractor & Trailer</strong>: 2Ã—64-channel LiDARs (360Â° FOV, 120m range, 10Hz) and 5 cameras (800Ã—600 resolution, 90Â°-120Â° FOV).</li>
                            <li><strong>CAV</strong>: 1Ã—64-channel LiDAR + 4 cameras (360Â° coverage).</li>
                            <li><strong>RSU</strong>: 1Ã—64-channel LiDAR + 1 camera (mounted at 4m height to avoid pedestrian blind spots).</li>
                        </ul>

                        <div class="figure">
                            <img src="images/sensor_layout.png" alt="Sensor Layout" class="figure-img">
                            <p class="figure-caption">Fig. 3: Sensor layout for tractor, trailer, CAV, and RSU.</p>
                        </div>
                    </div>

                    <!-- 2.3 Scenarios & Annotations -->
                    <div class="subsection">
                        <h3>Scenarios & Annotations</h3>
                        <p>
                            The dataset includes 64 scenarios generated in CARLA, covering diverse traffic environments with rich occlusion patterns:
                        </p>

                        <ul>
                            <li><strong>Scenario Types</strong>: T-intersections, roundabouts, highways, and urban streets with 35 vehicle types and 19 VRUs.</li>
                            <li><strong>Annotations</strong>: 3D bounding boxes with IDs, speeds, and 7 object categories (light/heavy vehicles, pedestrians, etc.).</li>
                            <li><strong>Splits</strong>: Training (38 scenarios), Validation (9), Testing (17).</li>
                        </ul>

                        <div class="figure">
                            <img src="images/scenarios.png" alt="Scenario Examples" class="figure-img">
                            <p class="figure-caption">Fig. 4: Representative scenarios with occlusion annotations (dashed ellipses).</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 3. Analysis of Occlusions -->
        <section id="occlusions-analysis" class="section">
            <div class="container">
                <h2>Analysis of Occlusions</h2>
                <div class="content">
                    <p>
                        Occlusions represent a critical challenge in truck perception due to the large size and articulated structure of commercial vehicles. This section provides a comprehensive analysis of occlusion patterns in TruckV2X, highlighting the unique challenges faced by autonomous trucking systems.
                    </p>

                    <div class="subsection">
                        <h3>Occlusion Classification</h3>
                        <p>
                            We categorize occlusions into three primary types based on their sources and characteristics:
                        </p>

                        <ul>
                            <li><strong>Self-occlusion</strong>: Caused by the truck's own structure, particularly between tractor and trailer during turning maneuvers. Accounts for 38% of all occlusions in urban scenarios.</li>
                            <li><strong>Inter-vehicle occlusion</strong>: Occurs between the truck and other vehicles, including passenger cars, buses, and other trucks. Represents 45% of total occlusions.</li>
                            <li><strong>Environmental occlusion</strong>: Caused by infrastructure elements such as buildings, trees, and barriers. Constitutes 17% of observed occlusions.</li>
                        </ul>

                        <div class="figure">
                            <img src="images/occlusion_types.png" alt="Types of Occlusions" class="figure-img">
                            <p class="figure-caption">Fig. 5: Visualization of three primary occlusion types in TruckV2X scenarios.</p>
                        </div>
                    </div>

                    <div class="subsection">
                        <h3>Occlusion Quantification</h3>
                        <p>
                            Our analysis reveals significant variations in occlusion patterns across different scenario types:
                        </p>

                        <ul>
                            <li><strong>Highway scenarios</strong> exhibit longer-duration occlusions (average 2.4s) due to higher speeds and dense traffic.</li>
                            <li><strong>Urban intersections</strong> show more frequent but shorter occlusions (average 0.8s) due to complex traffic interactions.</li>
                            <li><strong>Roundabouts</strong> present the highest occlusion density, with 72% of frames containing at least one partially occluded object.</li>
                        </ul>

                        <div class="figure">
                            <img src="images/occlusion_stats.png" alt="Occlusion Statistics" class="figure-img">
                            <p class="figure-caption">Fig. 6: Occlusion statistics across different scenario types in TruckV2X.</p>
                        </div>
                    </div>

                    <div class="subsection">
                        <h3>Impact on Perception</h3>
                        <p>
                            Occlusions significantly degrade perception performance, with our analysis showing:
                        </p>

                        <ul>
                            <li>Object detection accuracy drops by 43% for partially occluded objects and 78% for fully occluded objects using single-agent perception.</li>
                            <li>Trailer articulation angles greater than 15Â° increase self-occlusion area by 2.3Ã— compared to straight-line travel.</li>
                            <li>Heavy vehicles cause 2.7Ã— larger occlusion areas than passenger cars due to their larger profile.</li>
                        </ul>
                        <p>
                            These findings underscore the necessity of cooperative perception approaches for autonomous trucking, where multi-agent data fusion can mitigate occlusion effects.
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 4. Benchmark Experiments -->
        <section id="benchmarks" class="section section-alt">
            <div class="container">
                <h2>Benchmark Experiments</h2>
                <div class="content">
                    <p>
                        We evaluated 8 state-of-the-art cooperative perception methods on TruckV2X, focusing on occlusion handling performance:
                    </p>

                    <div class="subsection">
                        <h3>Key Findings</h3>
                        <ul>
                            <li><strong>Early Fusion</strong> outperforms other methods with mAP=47.88% (light vehicles, IoU=0.5) due to direct point cloud integration.</li>
                            <li><strong>Occlusion Recovery</strong>: Truck-CAV cooperation recovers >80% occluded objects in 60% of scenarios, proving trucks' role as mobile perception platforms.</li>
                            <li><strong>Turning Impact</strong>: Trailer articulation causes >70% occlusion, emphasizing the need for trailer-tractor cooperation.</li>
                        </ul>
                    </div>

                    <div class="figure">
                        <img src="images/benchmark_results.png" alt="Benchmark Results" class="figure-img">
                        <p class="figure-caption">Table 3: mAP comparison of cooperative perception methods.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 5. Citation -->
        <section id="citation" class="section">
            <div class="container">
                <h2>Citation</h2>
                <div class="citation-card">
                    <p>If you use TruckV2X in your research, please cite our paper:</p>
                    <pre class="bibtex">
@article{xie2025truckv2x,
  title={TruckV2X: A Truck-Centered Perception Dataset},
  author={Xie, Tenghui and Song, Zhiying and Wen, Fuxi and Li, Jun and Liu, Guangzhao and Zhao, Zijian},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  doi={10.1109/LRA.2025.XXXXXXX}
}
                    </pre>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>Â© 2025 TruckV2X Dataset | <a href="https://xietenghu1.github.io/">Contact</a></p>
        </div>
    </footer>
</body>
</html>
