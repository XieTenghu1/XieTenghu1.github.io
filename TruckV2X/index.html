<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TruckV2X Dataset</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <a href="#" class="logo">TruckV2X</a>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#dataset-construction">Dataset Construction</a>
                <a href="#occlusions-analysis">Occlusions Analysis</a>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#citation">Citation</a>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <h1 class="main-title">ðŸš› TruckV2X</h1>
            <p class="subtitle">The First Truck-Centered Cooperative Perception Dataset</p>
            
            <!-- Authors and Affiliations -->
            <div class="authors">
                <p>Tenghui XieÂ¹, Zhiying SongÂ¹, Fuxi WenÂ¹(Corresponding author), Jun LiÂ¹, Guangzhao LiuÂ², Zijian ZhaoÂ²</p>
                <div class="affiliations">
                    <div class="affiliation">
                        <span>Â¹School of Vehicle and Mobility, Tsinghua University</span>
                    </div>
                    <div class="affiliation">
                        <span>Â²FAW Jiefang Group Co., Ltd.</span>
                    </div>
                </div>
            </div>
            
            <div class="hero-buttons">
                <a href="https://arxiv.org/abs/2507.09505v1" class="btn">Paper (arXiv)</a>
                <a href="https://huggingface.co/datasets/XieTenghu1/TruckV2X" class="btn btn-secondary">Download Dataset</a>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <main>
        <!-- 1. Project Overview -->
        <section id="overview" class="section">
            <div class="container">
                <h2>Project Overview</h2>
                <div class="content">
                    <p>
                        Autonomous trucking faces unique perception challenges due to large vehicle size and dynamic trailer movements, leading to extensive blind spots and occlusions. Tractor-trailer systems create persistent blind zones (Figure 1a, 1b) where structural components obstruct both self-perception and neighboring agentsâ€™ sensing. Articulated motion exacerbates occlusions during maneuvers, while low-speed operation expands risk zones in mixed traffic.
                    </p>
                    <p>
                        Existing datasets (Figure 2) primarily focus on light vehicles, lacking multi-agent configurations for heavy-duty scenarios. To address this gap, we present <strong>TruckV2X</strong>â€”the first large-scale truck-centered cooperative perception dataset with multi-modal sensing (LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, RSUs). It supports the development of occlusion-resistant cooperative perception systems for autonomous trucking.
                    </p>

                    <!-- Figure 1: Truck-related occlusions -->
                    <div class="figure">
                        <img src="images/fig1.png" alt="Truck-related occlusions" class="figure-img">
                        <p class="figure-caption">Fig. 1: Illustration of truck-related occlusions. (a) Trailer obstructs tractor; (b) Truck obstructs others; (c) Occluded area at different positions within a 30m radius (tractor-trailer occludes 1.5Ã— more area than passenger cars).</p>
                    </div>

                    <!-- Figure 2: Dataset comparison -->
                    <div class="figure">
                        <img src="images/fig2.png" alt="Dataset comparison" class="figure-img">
                        <p class="figure-caption">Fig. 2: Datasets for autonomous driving perception. TruckV2X is the first multi-agent collaborative dataset for heavy vehicles (V2V: Vehicle-to-Vehicle, V2I: Vehicle-to-Infrastructure).</p>
                    </div>

                    <div class="key-points">
                        <div class="point">
                            <h3>Dataset Highlights</h3>
                            <p>â€¢ 88,396 LiDAR frames & 1M camera images<br>â€¢ 1.18M 3D bounding box annotations<br>â€¢ 64 scenarios covering urban/highway scenes<br>â€¢ Multi-agent data: tractors, trailers, CAVs, RSUs</p>
                        </div>
                        <div class="point">
                            <h3>Key Contributions</h3>
                            <p>â€¢ First truck-specific multi-agent cooperative dataset<br>â€¢ Benchmarks for occlusion handling tasks<br>â€¢ Quantifies trucks as both occlusion sources and perception enhancers</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 2. Dataset Construction -->
        <section id="dataset-construction" class="section section-alt">
            <div class="container">
                <h2>Dataset Construction</h2>
                <div class="content">
                    <!-- 2.1 Vehicle & Sensor Configurations -->
                    <div class="subsection">
                        <h3>Vehicle & Sensor Configurations</h3>
                        <p>
                            TruckV2X integrates a high-fidelity semi-trailer truck model (Table I) into CARLA via Unreal Engine, with physics-based kinematic constraints for realistic articulation. Each agent is equipped with multi-modal sensors:
                        </p>

                        <ul>
                            <li><strong>Tractor/Trailer</strong>: 2Ã—64-channel LiDARs (360Â° FOV, 120m range, 10Hz) and 5 cameras (800Ã—600 resolution, 90Â°-120Â° FOV).</li>
                            <li><strong>CAV</strong>: 1Ã—64-channel LiDAR + 4 cameras (360Â° coverage).</li>
                            <li><strong>RSU</strong>: 1Ã—64-channel LiDAR + 1 camera (mounted at 4m height to minimize pedestrian blind spots).</li>
                        </ul>

                        <!-- Figure 3: Sensor configurations -->
                        <div class="figure">
                            <img src="images/fig3.png" alt="Sensor configurations" class="figure-img">
                            <p class="figure-caption">Fig. 3: Sensor layouts for tractor, trailer, CAV, and RSU (LiDAR and camera ranges shown).</p>
                        </div>
                    </div>

                    <!-- 2.2 Scenarios & Data Statistics -->
                    <div class="subsection">
                        <h3>Scenarios & Data Statistics</h3>
                        <p>
                            The dataset was built using CARLA, with 64 scenarios generated by adding controlled random vehicles and pedestrians. Scenarios cover diverse environments: country roads, urban streets, highways, and intersections (T-shaped, four-way, roundabouts).
                        </p>

                        <!-- Figure 4: Representative scenarios -->
                        <div class="figure">
                            <img src="images/fig4.png" alt="Representative scenarios" class="figure-img">
                            <p class="figure-caption">Fig. 4: Representative scenarios (T-intersection, straight road, four-way intersection, roundabout) with occlusion annotations (dashed ellipses denote blind zones).</p>
                        </div>

                        <!-- Figure 5: Data statistics -->
                        <div class="figure">
                            <img src="images/fig5.png" alt="Data statistics" class="figure-img">
                            <p class="figure-caption">Fig. 5: Dataset statistics. (a) Frames per scenario; (b) Agent distances; (c) Agent speeds; (d) Tractor-trailer articulation angles; (e) Object categories (65% light vehicles, 20% VRUs, 15% heavy vehicles); (f) Co-visible objects; (g) Object-agent distances; (h) Object speeds.</p>
                        </div>

                        <p>
                            <strong>Dataset Splits</strong>: Training (38 scenarios, 13,243 frames), Validation (9 scenarios, 2,839 frames), Testing (17 scenarios, 6,017 frames).
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 3. Analysis of Occlusions -->
        <section id="occlusions-analysis" class="section">
            <div class="container">
                <h2>Analysis of Occlusions</h2>
                <div class="content">
                    <p>
                        Occlusions are a critical challenge in truck perception due to large size and articulation. TruckV2X captures extensive occlusion scenarios, with quantitative analysis revealing unique patterns:
                    </p>

                    <!-- Figure 6: Occlusion distribution -->
                    <div class="figure">
                        <img src="images/fig6.png" alt="Occlusion distribution" class="figure-img">
                        <p class="figure-caption">Fig. 6: Distribution of occluded objects in 120m range. (a) Truck's occluded objects; (b) CAV's objects occluded by truck; (c) RSU's objects occluded by truck; (d) Tractor's objects occluded by trailer (turning scenarios exacerbate occlusions).</p>
                    </div>

                    <div class="subsection">
                        <h3>Cooperation Benefits: Occlusion Recovery Rate (ORR)</h3>
                        <p>
                            Cooperative perception mitigates occlusions. ORR is defined as the proportion of an ego agentâ€™s occluded objects detected by cooperators (range 0-1). Key findings:
                        </p>
                        <ul>
                            <li>Truck-CAV cooperation recovers >80% occluded objects in 60% of scenarios (short distances improve performance).</li>
                            <li>RSUs provide stable mid-range support, while trucks act as mobile platforms to reduce blind spots during maneuvers.</li>
                        </ul>

                        <!-- Figure 7: ORR temporal evolution -->
                        <div class="figure">
                            <img src="images/fig7.png" alt="ORR temporal evolution" class="figure-img">
                            <p class="figure-caption">Fig. 7: Temporal evolution of ORR in a T-intersection scenario. Truck ORR decreases as CAV moves away, while RSU ORR increases (distance correlates with cooperation effectiveness).</p>
                        </div>

                        <!-- Figure 8: ORR distribution -->
                        <div class="figure">
                            <img src="images/fig8.png" alt="ORR distribution" class="figure-img">
                            <p class="figure-caption">Fig. 8: ORR frequency distribution for different agent configurations (truck, CAV, RSU) with distance probability densities.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 4. Benchmark Experiments -->
        <section id="benchmarks" class="section section-alt">
            <div class="container">
                <h2>Benchmark Experiments</h2>
                <div class="content">
                    <p>
                        We evaluated 8 state-of-the-art cooperative perception methods on TruckV2X, focusing on 3D object detection. Metrics include Average Precision (AP) and mean AP (mAP) at IoU thresholds 0.3, 0.5, 0.7 for light vehicles, heavy vehicles, and VRUs.
                    </p>

                    <div class="subsection">
                        <h3>Key Findings</h3>
                        <ul>
                            <li><strong>Early Fusion</strong> outperforms other methods (mAP=47.88% at IoU=0.5 for truck ego) due to direct point cloud integration.</li>
                            <li><strong>Intermediate Fusion</strong> (e.g., AttFuse, V2VNet) performs well for CAV ego, matching early fusion in light vehicle detection.</li>
                            <li>VRU detection remains challenging (low AP) due to small size and sparse LiDAR points.</li>
                        </ul>
                    </div>

                    <!-- Table III: Benchmark results (truck, CAV, RSU as ego) -->
                    <div class="table-container">
                        <h3>Table III: Detection Results (Truck, CAV, RSU as Ego)</h3>
                        <table class="benchmark-table">
                            <thead>
                                <tr>
                                    <th>Ego</th>
                                    <th>Method</th>
                                    <th>AP light @0.3</th>
                                    <th>AP light @0.5</th>
                                    <th>AP light @0.7</th>
                                    <th>mAP @0.5</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td rowspan="3">Truck</td>
                                    <td>Ego only</td>
                                    <td>51.16</td>
                                    <td>47.43</td>
                                    <td>33.31</td>
                                    <td>34.78</td>
                                </tr>
                                <tr>
                                    <td>Late Fusion</td>
                                    <td>72.15</td>
                                    <td>68.56</td>
                                    <td>53.27</td>
                                    <td>42.47</td>
                                </tr>
                                <tr>
                                    <td>Early Fusion</td>
                                    <td>77.22</td>
                                    <td>63.25</td>
                                    <td>49.17</td>
                                    <td>47.88</td>
                                </tr>
                                <tr>
                                    <td rowspan="3">CAV</td>
                                    <td>Ego only</td>
                                    <td>48.13</td>
                                    <td>46.01</td>
                                    <td>35.99</td>
                                    <td>28.04</td>
                                </tr>
                                <tr>
                                    <td>AttFuse</td>
                                    <td>77.54</td>
                                    <td>76.63</td>
                                    <td>69.43</td>
                                    <td>51.74</td>
                                </tr>
                                <tr>
                                    <td>Early Fusion</td>
                                    <td>79.10</td>
                                    <td>76.97</td>
                                    <td>67.08</td>
                                    <td>53.18</td>
                                </tr>
                            </tbody>
                        </table>
                        <p class="table-caption">Key subset of results (full results in the paper). Best performance in bold.</p>
                    </div>

                    <!-- Table IV: Benchmark results (tractor as ego with trailer) -->
                    <div class="table-container">
                        <h3>Table IV: Detection Results (Tractor as Ego with Trailer Cooperation)</h3>
                        <table class="benchmark-table">
                            <thead>
                                <tr>
                                    <th>Method</th>
                                    <th>AP heavy @0.3</th>
                                    <th>AP heavy @0.5</th>
                                    <th>AP heavy @0.7</th>
                                    <th>mAP @0.5</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>Ego only (tractor)</td>
                                    <td>62.63</td>
                                    <td>61.26</td>
                                    <td>56.28</td>
                                    <td>39.30</td>
                                </tr>
                                <tr>
                                    <td>Early Fusion (tractor+trailer)</td>
                                    <td>73.06</td>
                                    <td>72.19</td>
                                    <td>67.66</td>
                                    <td>49.16</td>
                                </tr>
                                <tr>
                                    <td>AttFuse</td>
                                    <td>69.76</td>
                                    <td>69.28</td>
                                    <td>64.83</td>
                                    <td>48.12</td>
                                </tr>
                            </tbody>
                        </table>
                        <p class="table-caption">Tractor-trailer cooperation significantly improves heavy vehicle detection.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 5. Citation -->
        <section id="citation" class="section">
            <div class="container">
                <h2>Citation</h2>
                <div class="citation-card">
                    <p>If you use TruckV2X in your research, please cite our paper:</p>
                    <pre class="bibtex">
@article{xie2025truckv2x,
  title={TruckV2X: A Truck-Centered Perception Dataset},
  author={Xie, Tenghui and Song, Zhiying and Wen, Fuxi and Li, Jun and Liu, Guangzhao and Zhao, Zijian},
  journal={IEEE Robotics and Automation Letters},
  year={2025},
  doi={10.1109/LRA.2025.XXXXXXX}
}
                    </pre>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p>Â© 2025 TruckV2X Dataset | <a href="https://xietenghu1.github.io/">Contact</a></p>
        </div>
    </footer>
</body>
</html>
